# -*- coding: utf-8 -*-
"""Text Classification & Combined Modelling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c9lj8SK1a6pKNZwtd-cyrZgnVoaTAjQF
"""

#Final Random Forests model

import pandas as pd
import re
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import LabelEncoder
import nltk

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

# Function to clean and preprocess text
def preprocess_text(text):
    # Lowercasing
    text = text.lower()

    # Remove non-alphanumeric characters
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = re.sub(r'\*', '', text)

    # Tokenization
    tokens = word_tokenize(text)

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [word for word in tokens if word not in stop_words]

    # Stemming and Lemmatization
    stemmer = PorterStemmer()
    lemmatizer = WordNetLemmatizer()
    preprocessed_text = ' '.join([lemmatizer.lemmatize(stemmer.stem(token)) for token in filtered_tokens])

    return preprocessed_text

# Data loading
df_labelled = pd.read_csv("Products.csv")

df_labelled['Product ID'] = df_labelled['Link'].str.extract(r'/images/I/(.+?)\._')

# Apply preprocessing
df_labelled['Name'] = df_labelled['Name'].apply(preprocess_text)
# Convert 'Category' to lowercase and strip spaces
df_labelled['Tag'] = df_labelled['Tag'].str.lower().str.strip()
df_labelled_clean = df_labelled[['Tag', 'Name', 'Product ID']]

# Split the data (keeping Product ID separate)
X = df_labelled_clean['Name']
y = df_labelled_clean['Tag']
ids = df_labelled_clean['Product ID']

X_train, X_test, y_train, y_test, ids_train, ids_test = train_test_split(X, y, ids, test_size=0.2)

# Vectorize the text data
vectorizer = CountVectorizer(max_features=5000)
X_train = vectorizer.fit_transform(X_train)
X_test = vectorizer.transform(X_test)

# Encode labels
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_test = label_encoder.transform(y_test)

# Train a Random Forest Classifier
random_forest = RandomForestClassifier(n_estimators=100, random_state=42)
random_forest.fit(X_train, y_train)

# Evaluate the model
accuracy = random_forest.score(X_test, y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

# Load the holdout dataset
holdout_df = pd.read_csv("Final test.csv")

# Concatenate 'Title' and 'Description' and preprocess text
holdout_df['Combined_Text'] = holdout_df['Title'] + ' ' + holdout_df['Description']
holdout_df['Combined_Text'] = holdout_df['Combined_Text'].apply(preprocess_text)
holdout_df['Product ID'] = holdout_df['Image URL'].str.extract(r'.org/(.*)_600x450\.jpg')
holdout_data = vectorizer.transform(holdout_df['Combined_Text'])

# Predict categories using the trained model
holdout_predictions = random_forest.predict(holdout_data)
# Get prediction probabilities for each class
holdout_probabilities = random_forest.predict_proba(holdout_data)


# Convert numerical labels back to class labels
predicted_classes = label_encoder.inverse_transform(holdout_predictions)

# Add the predicted classes to the holdout DataFrame
holdout_df['Predicted_Class'] = predicted_classes
holdout_df['Prediction_Probabilities'] = [max(probabilities) for probabilities in holdout_probabilities]


# Save the holdout DataFrame with predictions to a CSV file
holdout_df.to_csv('holdout_predictions.csv', index=False)

# Calculate accuracy on the holdout dataset
holdout_labels = holdout_df['Product']  # Actual labels in the holdout dataset
holdout_accuracy = (holdout_labels == predicted_classes).mean() * 100  # Calculate accuracy

print(f"Accuracy on the holdout dataset: {holdout_accuracy:.2f}%")

# Access the parameters of the trained model
model_params = random_forest.get_params()
print(model_params)

"""Importing the holdout resultset for image Classification"""

# Load the holdout dataset
image_data = pd.read_csv("rf_data (1).csv")

# Renaming columns
image_data = image_data.rename(columns={
    'predicted_label': 'Image Class',
    'probability': 'Image Prob',
    # Add more columns as needed
})

# Renaming columns
holdout_df = holdout_df.rename(columns={
    'Predicted_Class': 'Text Class',
    'Prediction_Probabilities': 'Text Prob',
    # Add more columns as needed
})

# Joining the DataFrames on 'Product ID'
merged_df = holdout_df.merge(image_data, on='Product ID', how='inner')

# List of columns to remove (replace with your column names)
columns_to_remove = ['Product ID', 'Unnamed: 0']

# Remove the specified columns
merged_df = merged_df.drop(columns=columns_to_remove)

# Remove duplicate rows across all columns
merged_df = merged_df.drop_duplicates()

# Reset the index after removing rows
merged_df = merged_df.reset_index(drop=True)

# Display the merged DataFrame
merged_df.head()

"""Calculating the combined proability of each product using both Text & Image Classification"""

# 1. Convert 'Image Class' to lowercase and strip spaces
merged_df['Image Class'] = merged_df['Image Class'].str.lower().str.strip()

# 2. Create 'Comb_Prob' and 'Comb_Class'
merged_df['Comb_Prob'] = np.maximum(merged_df['Text Prob'], merged_df['Image Prob'])
merged_df['Comb_Class'] = np.where(merged_df['Comb_Prob'] == merged_df['Text Prob'], merged_df['Text Class'], merged_df['Image Class'])

# 3. Calculate Misclassification Rate
misclassified = merged_df['Product_x'] != merged_df['Comb_Class']
misclassification_rate = misclassified.mean() * 100  # In percentage

print(f"Misclassification Rate combining both text and image classification models: {misclassification_rate:.2f}%")

# Save the DataFrame to a CSV file
merged_df.to_csv("merged_data.csv", index=False)

"""# **Other noteworthy models tried**"""

#LSTM Model
#Final model: LSTM with 100 neurons

import pandas as pd
import re
import numpy as np
import tensorflow
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import nltk

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Function to clean and preprocess text
def preprocess_text(text):
    # Lowercasing
    text = text.lower()

    # Remove non-alphanumeric characters
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = re.sub(r'\*', '', text)

    # Tokenization
    tokens = word_tokenize(text)

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [word for word in tokens if word not in stop_words]

    # Stemming and Lemmatization
    stemmer = PorterStemmer()
    lemmatizer = WordNetLemmatizer()
    preprocessed_text = ' '.join([lemmatizer.lemmatize(stemmer.stem(token)) for token in filtered_tokens])

    return preprocessed_text

# Data loading
df_labelled = pd.read_csv("Products.csv")

df_labelled['Product ID'] = df_labelled['Link'].str.extract(r'/images/I/(.+?)\._')

# Apply preprocessing
df_labelled['Name'] = df_labelled['Name'].apply(preprocess_text)
# Convert 'Category' to lowercase and strip spaces
df_labelled['Tag'] = df_labelled['Tag'].str.lower().str.strip()
df_labelled_clean = df_labelled[['Tag', 'Name', 'Product ID']]

# Tokenize text data
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(df_labelled_clean['Name'])

# Convert text to sequence of integers
sequences = tokenizer.texts_to_sequences(df_labelled_clean['Name'])
data = pad_sequences(sequences, maxlen=200)

# Encode labels
label_encoder = LabelEncoder()
encoded_labels = label_encoder.fit_transform(df_labelled_clean['Tag'])
labels = tensorflow.keras.utils.to_categorical(encoded_labels)

# Split the data (keeping Product ID separate)
X_train, X_test, y_train, y_test, ids_train, ids_test = train_test_split(data, labels, df_labelled_clean['Product ID'], test_size=0.2)

# Model architecture
model = Sequential()
model.add(Embedding(input_dim=5000, output_dim=64, input_length=200))
model.add(LSTM(100)) # Running with 100 neurons
model.add(Dense(len(label_encoder.classes_), activation='softmax'))

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=32)

# Display the model parameters
model_params = model.get_config()
print("Model Parameters:")
print(model_params)

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

# Predict class probabilities on the test set
probabilities = model.predict(X_test)

# Initialize an empty list to store the results
results = []

# Loop through the predictions and store each result in the list
for i, probability in enumerate(probabilities):
    # Find the class with the highest probability
    predicted_class_index = probability.argmax()
    predicted_class = label_encoder.classes_[predicted_class_index]
    predicted_probability = probability[predicted_class_index]

    # Retrieve the original text and Product ID
    product_id = ids_test.iloc[i]
    original_text = df_labelled_clean[df_labelled_clean['Product ID'] == product_id]['Name'].iloc[0]

    # Append the result as a dictionary to the results list
    results.append({
        "Product ID": product_id,
        "Text": original_text,
        "Predicted Class": predicted_class,
        "Probability": predicted_probability
    })


#Final predictions for holdout

# Load the holdout dataset
holdout_df = pd.read_csv("Final test.csv")

# Concatenate 'Title' and 'Description' and preprocess text
holdout_df['Combined_Text'] = holdout_df['Title'] + ' ' + holdout_df['Description']
holdout_df['Combined_Text'] = holdout_df['Combined_Text'].apply(preprocess_text)
holdout_df['Product ID'] = holdout_df['Image URL'].str.extract(r'.org/(.*)_600x450\.jpg')

# Tokenize and pad text data in the holdout dataset
holdout_sequences = tokenizer.texts_to_sequences(holdout_df['Combined_Text'])
holdout_data = pad_sequences(holdout_sequences, maxlen=200)

# Strip spaces and filter out unseen labels
holdout_df['Product'] = holdout_df['Product'].str.strip()

# removes the labels that were not in the training dataset
known_labels = set(label_encoder.classes_)
filtered_holdout_df = holdout_df[holdout_df['Product'].isin(known_labels)]

# Encode labels for the filtered data
filtered_holdout_labels = label_encoder.transform(filtered_holdout_df['Product'])
filtered_holdout_labels = tensorflow.keras.utils.to_categorical(filtered_holdout_labels, num_classes=len(label_encoder.classes_))

# Predict categories using the trained model
filtered_holdout_data = holdout_data[holdout_df['Product'].isin(known_labels)]
holdout_predictions = model.predict(filtered_holdout_data)

# Calculate accuracy
predicted_class_indices = np.argmax(holdout_predictions, axis=1)
true_class_indices = np.argmax(filtered_holdout_labels, axis=1)
accuracy = np.mean(predicted_class_indices == true_class_indices)
print(f"Accuracy on the filtered holdout dataset: {accuracy * 100:.2f}%")

# Initialize an empty list to store the results
results = []

# Loop through the predictions and store each result in the list
for i, probability in enumerate(holdout_predictions):
    # Find the class with the highest probability
    predicted_class_index = probability.argmax()
    predicted_class = label_encoder.classes_[predicted_class_index]
    predicted_probability = probability[predicted_class_index]

    # Retrieve the original combined text and Product ID
    original_text = filtered_holdout_df.iloc[i]['Combined_Text']
    product_id = filtered_holdout_df.iloc[i]['Product ID']

    # Append the result as a dictionary to the results list
    results.append({
        "Product ID": product_id,
        "Text": original_text,
        "Predicted Class": predicted_class,
        "Probability": predicted_probability
    })

# Convert the list of dictionaries into a DataFrame
results_df = pd.DataFrame(results)

# Display the DataFrame
print(results_df)
import pandas as pd

# Specify the file path where you want to save the CSV file
file_path = 'results_holdout.csv'

# Use the to_csv method to save the DataFrame as a CSV file
results_df.to_csv(file_path, index=False)  # Setting index=False to omit writing row numbers as an extra column

import pandas as pd
import re
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
import nltk

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

# Function to clean and preprocess text
def preprocess_text(text):
    # Lowercasing
    text = text.lower()
    # Remove non-alphanumeric characters
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Tokenization
    tokens = word_tokenize(text)
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [word for word in tokens if word not in stop_words]
    # Stemming and Lemmatization
    stemmer = PorterStemmer()
    lemmatizer = WordNetLemmatizer()
    preprocessed_text = ' '.join([lemmatizer.lemmatize(stemmer.stem(token)) for token in filtered_tokens])
    return preprocessed_text

# Data loading
df_labelled = pd.read_csv("Products.csv")
# Extract 'Product ID' from 'Link' column if required
# df_labelled['Product ID'] = df_labelled['Link'].str.extract(r'/images/I/(.+?)\._')

# Apply preprocessing
df_labelled['Name'] = df_labelled['Name'].apply(preprocess_text)
# Convert 'Category' to lowercase and strip spaces
df_labelled['Tag'] = df_labelled['Tag'].str.lower().str.strip()
df_labelled_clean = df_labelled[['Tag', 'Name']]  # Assuming 'Product ID' is not needed for training

# Split the data
X = df_labelled_clean['Name']
y = df_labelled_clean['Tag']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Vectorize the text data
vectorizer = CountVectorizer(max_features=5000)
X_train = vectorizer.fit_transform(X_train)
X_test = vectorizer.transform(X_test)

# Encode labels
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_test = label_encoder.transform(y_test)

# Define a list of models to train and evaluate
models = [
    ('RandomForest', RandomForestClassifier(n_estimators=100, random_state=42)),
    ('LogisticRegression', LogisticRegression(max_iter=1000)),
    ('MultinomialNB', MultinomialNB()),
    ('SVM', SVC(probability=True)),
]

# Train and evaluate models
model_results = []
for model_name, model in models:
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    model_results.append((model_name, accuracy * 100))

# Convert model results to a DataFrame
model_summary_df = pd.DataFrame(model_results, columns=['Model', 'Test Accuracy (%)'])

# Display the model summary DataFrame
print(model_summary_df)

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import LabelEncoder

# Assuming these functions are defined earlier in your code:
# preprocess_text, vectorizer, label_encoder, and your models

# Load and preprocess the holdout dataset
holdout_df = pd.read_csv("Final test.csv")

# Concatenate 'Title' and 'Description' and preprocess text
holdout_df['Combined_Text'] = holdout_df['Title'] + ' ' + holdout_df['Description']
holdout_df['Combined_Text'] = holdout_df['Combined_Text'].apply(preprocess_text)
holdout_df['Product ID'] = holdout_df['Image URL'].str.extract(r'.org/(.*)_600x450\.jpg')

# Strip spaces and convert to lowercase
holdout_df['Product'] = holdout_df['Product'].str.strip().str.lower()

# Filter out unseen labels from the holdout DataFrame
known_labels = set(label_encoder.classes_)
filtered_holdout_df = holdout_df[holdout_df['Product'].isin(known_labels)]

# Vectorize the filtered holdout dataset text
holdout_X = vectorizer.transform(filtered_holdout_df['Combined_Text'])

# Encode the filtered holdout dataset labels
holdout_y = label_encoder.transform(filtered_holdout_df['Product'])

# Initialize a DataFrame to store holdout results
holdout_results = []

# Evaluate models on the filtered holdout dataset
for model_name, model in models:
    holdout_pred = model.predict(holdout_X)
    holdout_accuracy = accuracy_score(holdout_y, holdout_pred)
    holdout_results.append((model_name, holdout_accuracy * 100))

# Convert holdout results to a DataFrame
holdout_summary_df = pd.DataFrame(holdout_results, columns=['Model', 'Holdout Accuracy (%)'])

# Display the holdout summary DataFrame
print(holdout_summary_df)